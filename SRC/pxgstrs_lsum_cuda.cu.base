/*! \file
Copyright (c) 2003, The Regents of the University of California, through
Lawrence Berkeley National Laboratory (subject to receipt of any required
approvals from U.S. Dept. of Energy)

All rights reserved.

The source code is distributed under BSD license, see the file License.txt
at the top-level directory.
*/
@extract -b Typedef.inc
/*! @file
 * \brief Perform local block modifications: lsum[i] -= L_i,k * X[k]
 *
 * <pre>
 * -- Distributed SuperLU routine (version 7.1.0) --
 * Lawrence Berkeley National Lab, Univ. of California Berkeley.
 * March 15, 2003
 *
 * Modified:
 *     Feburary 7, 2001    use MPI_Isend/MPI_Irecv
 *     October 2, 2001     use MPI_Isend/MPI_Irecv with MPI_Test
 *     February 8, 2019  version 6.1.1
 *     October 5, 2021   version 7.1.0  disable a few 'omp simd'
 * </pre>
 */
#include <math.h>
#include "superlu_@(pre)defs.h"
#include "superlu_defs.h"

#ifndef CACHELINE
#define CACHELINE 64  /* bytes, Xeon Phi KNL, Cori haswell, Edision */
#endif


#ifndef MAXSUPER
////#define MAXSUPER 5
#define MAXSUPER 256
#endif


#include <stdio.h>
#include "mpi.h"
#include <nvshmem.h>
#include <nvshmemx.h>
#include <stdlib.h>
#include <sched.h>
#include <nvml.h>
#include <omp.h>
#include <cooperative_groups.h>

#undef CUDA_CHECK
#define CUDA_CHECK(stmt)                                                          \
     do {                                                                          \
         cudaError_t result = (stmt);                                              \
         if (cudaSuccess != result) {                                              \
             fprintf(stderr, "[%s:%d] cuda failed with %s \n", __FILE__, __LINE__, \
                     cudaGetErrorString(result));                                  \
             exit(-1);                                                             \
         }                                                                         \
         assert(cudaSuccess == result);                                            \
     } while (0)

#undef MPI_CHECK
#define MPI_CHECK(stmt)                                 \
 do {                                                    \
     int result = (stmt);                                \
     if (MPI_SUCCESS != result) {                        \
         fprintf(stderr, "[%s:%d] MPI failed with error %d \n",\
          __FILE__, __LINE__, result);                   \
         exit(-1);                                       \
     }                                                   \
 } while (0)

#define NVSHMEM_CHECK(stmt)                               \
 do {                                                    \
     int result = (stmt);                                \
     if (cudaSuccess != result) {                      \
         fprintf(stderr, "[%s:%d] nvshmem failed with error %d \n",\
          __FILE__, __LINE__, result);                   \
         exit(-1);                                       \
     }                                                   \
 } while (0)

#define cudaCheckError() { \
    cudaError_t e=cudaGetLastError();                           \
    if(e!=cudaSuccess) {                       \
        printf("Cuda failure %s:%d: '%s'\n",__FILE__,__LINE__,cudaGetErrorString(e));                           \
        exit(EXIT_FAILURE);                   \
    }                       \
}

/* Macro definitions */

/*! \brief Complex Copy c = a */
#define z_copy(c, a) { (c)->r = (a)->r ; \
             (c)->i = (a)->i ; }

/*! \brief Complex Addition c = a + b */
#define z_add(c, a, b) { (c)->r = (a)->r + (b)->r; \
             (c)->i = (a)->i + (b)->i; }

/*! \brief Complex Subtraction c = a - b */
#define z_sub(c, a, b) { (c)->r = (a)->r - (b)->r; \
             (c)->i = (a)->i - (b)->i; }

/*! \brief Complex-Double Multiplication */
#define zd_mult(c, a, b) { (c)->r = (a)->r * (b); \
                           (c)->i = (a)->i * (b); }

/*! \brief Complex-Complex Multiplication */
#define zz_mult(c, a, b) { \
    double cr, ci; \
        cr = (a)->r * (b)->r - (a)->i * (b)->i; \
        ci = (a)->i * (b)->r + (a)->r * (b)->i; \
        (c)->r = cr; \
        (c)->i = ci; \
    }

/*! \brief Complex equality testing */
#define z_eq(a, b)  ( (a)->r == (b)->r && (a)->i == (b)->i )


#ifdef __cplusplus
extern "C" {
#endif

/***************************************************************************//**
	 Does sum reduction of n-element array x, leaving total in x[0].
	 Contents of x are destroyed in the process.
	 With k threads, can reduce array up to 2*k in size.
	 Assumes number of threads <= 1024 (which is max number of threads up to CUDA capability 3.0)
	 Having n as template parameter allows compiler to evaluate some conditions at compile time.
	 Calls __syncthreads before & after reduction.
	 @ingroup magma_kernel
 *******************************************************************************/
__device__ void
magma_sum_reduce( int n, int i, double* x )
{
    __syncthreads();
    if ( n > 1024 ) { if ( i < 1024 && i + 1024 < n ) { x[i] += x[i+1024]; }  __syncthreads(); }
    if ( n >  512 ) { if ( i <  512 && i +  512 < n ) { x[i] += x[i+ 512]; }  __syncthreads(); }
    if ( n >  256 ) { if ( i <  256 && i +  256 < n ) { x[i] += x[i+ 256]; }  __syncthreads(); }
    if ( n >  128 ) { if ( i <  128 && i +  128 < n ) { x[i] += x[i+ 128]; }  __syncthreads(); }
    if ( n >   64 ) { if ( i <   64 && i +   64 < n ) { x[i] += x[i+  64]; }  __syncthreads(); }
    if ( n >   32 ) { if ( i <   32 && i +   32 < n ) { x[i] += x[i+  32]; }  __syncthreads(); }
    // probably don't need __syncthreads for < 16 threads
    // because of implicit warp level synchronization.
    if ( n >   16 ) { if ( i <   16 && i +   16 < n ) { x[i] += x[i+  16]; }  __syncthreads(); }
    if ( n >    8 ) { if ( i <    8 && i +    8 < n ) { x[i] += x[i+   8]; }  __syncthreads(); }
    if ( n >    4 ) { if ( i <    4 && i +    4 < n ) { x[i] += x[i+   4]; }  __syncthreads(); }
    if ( n >    2 ) { if ( i <    2 && i +    2 < n ) { x[i] += x[i+   2]; }  __syncthreads(); }
    if ( n >    1 ) { if ( i <    1 && i +    1 < n ) { x[i] += x[i+   1]; }  __syncthreads(); }
}
// end sum_reduce



/******************************************************************************/
static __device__ void
gemv_device_dlsum_fmod(
        int_t m, int_t n, double alpha,
        const double * __restrict__ A, int_t lda,
        const double * __restrict__ x, int_t incx, double beta,
        double       * __restrict__ y, int_t incy)
{
    if (m <= 0 || n <= 0) return;

    int_t num_threads = DIM_X * DIM_Y;
    int_t thread_id = threadIdx_x + threadIdx_y * blockDim_x;

    // threads are all configurated locally
    int_t tx = thread_id % DIM_X;
    int_t ty = thread_id / DIM_X;

    int_t ind = tx;

    __shared__ double sdata[DIM_X * DIM_Y];


    int_t st = 0;

    int_t ed = min(st+m, CEILING(m,DIM_X)*DIM_X);

    int_t iters = CEILING(ed-st,DIM_X) ;

    double zero = 0.0;

    for (int_t i=0; i < iters; i++)
    {
        if (ind < m ) A += ind;

        double res = zero;

        if (ind < m )
        {
            for (int_t col=ty; col < n; col += DIM_Y)
            {
                res += A[col*lda] * x[col*incx];
            }
        }

        if (DIM_X >= num_threads) // indicated 1D threads configuration. Shared memory is not needed, reduction is done naturally
        {
            if (ty == 0 && ind < m)
            {
                y[ind*incy] = alpha*res + beta*y[ind*incy];
            }
        }
        else
        {
            sdata[ty + tx * DIM_Y] = res;

            __syncthreads();

            if ( DIM_Y > 16)
            {
                magma_sum_reduce(DIM_Y, ty, sdata + tx * DIM_Y);
            }
            else
            {
                if (ty == 0 && ind < m)
                {
                    for (int_t i=1; i < DIM_Y; i++)
                    {
                        sdata[tx * DIM_Y] += sdata[i + tx * DIM_Y];
                    }
                }
            }

            if (ty == 0 && ind < m)
            {
                y[ind*incy] = alpha*sdata[tx * DIM_Y] + beta*y[ind*incy];
            }

            __syncthreads();
        }

        if ( ind < m) A -= ind;

        ind += DIM_X;
    }
}





/******************************************************************************/
static __device__
void gemm_device_dlsum_fmod(
        int_t M, int_t N, int_t K,
        int_t blx, int_t bly,
        const @(type)* __restrict__ A, int_t LDA,
        const @(type)* __restrict__ B, int_t LDB,
        @(type) rC[THR_N][THR_M],
        @(type) alpha, @(type) beta)
{
    // #if (__CUDA_ARCH__ >= 200)
    int_t idx = threadIdx_x;  // thread's m dimension
    int_t idy = threadIdx_y;  // thread's n dimension

    int_t idt = DIM_X * idy + idx;    // thread's global number

    int_t idxA = idt % DIM_XA;    // idx within A
    int_t idyA = idt / DIM_XA;    // idy within A

    int_t idxB = idt % DIM_XB;    // idx within B
    int_t idyB = idt / DIM_XB;    // idy within B

    // int_t blx = blockIdx_x;   // block's m dimension
    // int_t bly = blockIdx_y;   // block's n dimension

    __shared__ @(type) sA[BLK_K][BLK_M+1];      // +1 only required if A is transposed
    __shared__ @(type) sB[BLK_N][BLK_K+1];      // +1 always required

    // Registers for the innermost loop
    @(type) rA[THR_M];
    @(type) rB[THR_N];

    @(type) ra[BLK_K/DIM_YA+1][BLK_M/DIM_XA];
    @(type) rb[BLK_N/DIM_YB][BLK_K/DIM_XB+1];

    const double *offs_dA = A + blx*BLK_M     + idyA*LDA + idxA;
    const double *offs_dB = B + bly*BLK_N*LDB + idyB*LDB + idxB;
    int_t boundA = (LDA*(K-1) + M) - ( blx*BLK_M  + idyA*LDA + idxA ) -1;
    int_t boundB = (LDB*(N-1) + K) - ( bly*BLK_N*LDB + idyB*LDB + idxB ) -1;

    int_t m, n, k, kk;
@precision SINGLE DOUBLE
    @(type) zero = 0.0;
@precision SCOMPLEX DCOMPLEX
    @(type) zero = {0.0, 0.0};
@precision !

    // Zero C
#pragma unroll
    for (n = 0; n < THR_N; n++)
#pragma unroll
            for (m = 0; m < THR_M; m++){
@precision SINGLE DOUBLE
                rC[n][m] = zero;
@precision SCOMPLEX DCOMPLEX
                z_copy(&rC[n][m],&zero);
@precision !
                }

#pragma unroll
    for (n = 0; n < BLK_K; n += DIM_YA)
#pragma unroll
            for (m = 0; m < BLK_M; m += DIM_XA)
@precision SINGLE DOUBLE
                sA[n+idyA][m+idxA] = fetch(A, m, n, boundA);
@precision SCOMPLEX DCOMPLEX
                z_copy(&sA[n+idyA][m+idxA], &fetch(A, m, n, boundA));
@precision !

#pragma unroll
    for (n = 0; n < BLK_N; n += DIM_YB)
#pragma unroll
            for (m = 0; m < BLK_K; m += DIM_XB)
@precision SINGLE DOUBLE
                sB[n+idyB][m+idxB] = fetch(B, m, n, boundB);
@precision SCOMPLEX DCOMPLEX
                z_copy(sB[n+idyB][m+idxB] = fetch(B, m, n, boundB));
@precision !


    __syncthreads();

    for (kk = 0; kk < K-BLK_K; kk += BLK_K)
    {
        offs_dA += BLK_K*LDA;
        boundA  -= BLK_K*LDA;

        offs_dB += BLK_K;
        boundB  -= BLK_K;

#pragma unroll
        for (n = 0; n < BLK_K/DIM_YA; n++)
#pragma unroll
                for (m = 0; m < BLK_M/DIM_XA; m++)
@precision SINGLE DOUBLE
                    ra[n][m] = fetch(A, m*DIM_XA, n*DIM_YA, boundA);
@precision SCOMPLEX DCOMPLEX
                    z_copy(&ra[n][m],&fetch(A, m*DIM_XA, n*DIM_YA, boundA));
 @precision !


#pragma unroll
        for (n = 0; n < BLK_N/DIM_YB; n++)
#pragma unroll
                for (m = 0; m < BLK_K/DIM_XB; m++)
@precision SINGLE DOUBLE
                    rb[n][m] = fetch(B, m*DIM_XB, n*DIM_YB, boundB);
@precision SCOMPLEX DCOMPLEX
                    z_copy(&ra[n][m], &fetch(A, m*DIM_XA, n*DIM_YA, boundA));
@precision !

        // Multiply
#pragma unroll
        for (k = 0; k < BLK_K; k++)
        {
            // Load A shmem->regs
#pragma unroll
            for (m = 0; m < THR_M; m++)
@precision SINGLE DOUBLE
                rA[m] = sA[k][m*DIM_X+idx];
@precision SCOMPLEX DCOMPLEX
                z_copy(&rA[m],&sA[k][m*DIM_X+idx]);
@precision !

            // Load B shmem->regs
#pragma unroll
            for (n = 0; n < THR_N; n++)
@precision SINGLE DOUBLE
                rB[n] = sB[n*DIM_Y+idy][k];
@precision SCOMPLEX DCOMPLEX
                z_copy(&rB[n], &sB[n*DIM_Y+idy][k]);
@precision !

            // Compute
#pragma unroll
            for (n = 0; n < THR_N; n++) {
#pragma unroll
                for (m = 0; m < THR_M; m++) {
                    fma(rA[m], rB[n], rC[n][m]);
                }
            }
        }

        __syncthreads();

#pragma unroll
        for (n = 0; n < BLK_K/DIM_YA; n++)
#pragma unroll
                for (m = 0; m < BLK_M/DIM_XA; m++)
@precision SINGLE DOUBLE
                    sA[n*DIM_YA+idyA][m*DIM_XA+idxA] = ra[n][m];
@precision SCOMPLEX DCOMPLEX
                    z_copy(&sA[n*DIM_YA+idyA][m*DIM_XA+idxA], &ra[n][m]);
@precision !


#pragma unroll
        for (n = 0; n < BLK_N/DIM_YB; n++)
#pragma unroll
                for (m = 0; m < BLK_K/DIM_XB; m++)
@precision SINGLE DOUBLE
                    sB[n*DIM_YB+idyB][m*DIM_XB+idxB] = rb[n][m];
@precision SCOMPLEX DCOMPLEX
                    z_copy(&sB[n*DIM_YB+idyB][m*DIM_XB+idxB], &rb[n][m]);
@precision !

        __syncthreads();
    }

    // Multiply last full (BLK_K) or partial block of
    // columns of op(A) and rows of op(B).
    // It's okay that m,n exceed matrix bounds as all work is in registers
    // or shared memory, and out-of-bounds rC[n][m] will not be saved later.
    kk = K - kk;
#pragma unroll
    for (k = 0; k < kk; k++)
    {
        // Load A shmem->regs
#pragma unroll
        for (m = 0; m < THR_M; m++)
@precision SINGLE DOUBLE
            rA[m] = sA[k][m*DIM_X+idx];
@precision SCOMPLEX DCOMPLEX
            z_copy(&rA[m], &sA[k][m*DIM_X+idx]);
@precision !

        // Load B shmem->regs
#pragma unroll
        for (n = 0; n < THR_N; n++)
@precision SINGLE DOUBLE
            rB[n] = sB[n*DIM_Y+idy][k];
@precision SCOMPLEX DCOMPLEX
            z_copy(&rB[n],&sB[n*DIM_Y+idy][k]);
@precision !

        // Compute
#pragma unroll
        for (n = 0; n < THR_N; n++) {
#pragma unroll
            for (m = 0; m < THR_M; m++) {
                fma(rA[m], rB[n], rC[n][m]);
            }
        }
    }

    // Store C regs->dev
    // if( beta == make_FloatingPoint_t(0.0,0.0) ) {
    // #pragma unroll
    // for (n = 0; n < THR_N; n++) {
    // int_t coord_dCn = bly*BLK_N + n*DIM_Y + idy;
    // #pragma unroll
    // for (m = 0; m < THR_M; m++) {
    // int_t coord_dCm = blx*BLK_M + m*DIM_X + idx;
    // if (coord_dCm < M && coord_dCn < N) {
    // int_t offsC = coord_dCn*LDC + coord_dCm;

    // double &regC = rC[n][m];
    // double &memC = C[offsC];

    // // memC = mul(alpha, regC);
    // }
    // }
    // }
    // } else {
    // #pragma unroll
    // for (n = 0; n < THR_N; n++) {
    // int_t coord_dCn = bly*BLK_N + n*DIM_Y + idy;
    // #pragma unroll
    // for (m = 0; m < THR_M; m++) {
    // int_t coord_dCm = blx*BLK_M + m*DIM_X + idx;
    // if (coord_dCm < M && coord_dCn < N) {
    // int_t offsC = coord_dCn*LDC + coord_dCm;

    // double &regC = rC[n][m];
    // double &memC = C[offsC];

    // // memC = add(mul(alpha, regC), mul(beta, memC));
    // }
    // }
    // }
    // }
    // #endif /* (__CUDA_ARCH__ >= 200) */
}



void nv_init_wrapper(int* c, char *v[], int* omp_mpi_level)
{
    int *target;
    int rank, nranks, ndevices;
    MPI_Comm mpi_comm;
    nvshmemx_init_attr_t attr;
    int mype, npes, mype_node;
    //MPI_CHECK(MPI_Init(&c, &v));
    MPI_CHECK(MPI_Init_thread( c, &v, MPI_THREAD_MULTIPLE, omp_mpi_level));
    MPI_CHECK(MPI_Comm_rank(MPI_COMM_WORLD, &rank));
    MPI_CHECK(MPI_Comm_size(MPI_COMM_WORLD, &nranks));


    //CUDA_CHECK(cudaSetDevice(rank%ndevices));
    mpi_comm = MPI_COMM_WORLD;
    attr.mpi_comm = &mpi_comm;
    NVSHMEM_CHECK(nvshmemx_init_attr (NVSHMEMX_INIT_WITH_MPI_COMM, &attr));
    mype = nvshmem_my_pe();
    npes = nvshmem_n_pes();

    mype_node = nvshmem_team_my_pe(NVSHMEMX_TEAM_NODE);
    char name[MPI_MAX_PROCESSOR_NAME];
    int resultlength;
    MPI_CHECK(MPI_Get_processor_name(name, &resultlength));
    int get_cur_dev;
    CUDA_CHECK(cudaGetDeviceCount(&ndevices));
    //CUDA_CHECK(cudaSetDevice(rank%ndevices));
    CUDA_CHECK(cudaSetDevice(mype_node));
    CUDA_CHECK(cudaGetDevice(&get_cur_dev));

    cudaDeviceProp prop;
    //CUDA_CHECK(cudaGetDeviceProperties(&prop, rank%ndevices));
    CUDA_CHECK(cudaGetDeviceProperties(&prop, mype_node));
    printf("** MPI %d/%d, NVSHMEM %d/%d, mype_node=%d, device name: %s bus id: %d, "
           "ndevices=%d,cur=%d, node=%s **\n",
           rank,nranks,mype,npes,mype_node, prop.name, prop.pciBusID,
           ndevices,get_cur_dev,name);
    fflush(stdout);

}

__device__ void C_BcTree_forwardMessageSimple_Device(C_Tree* tree,  int* flag_bc_q,  int* my_flag_bc, int mype, int tid,double* ready_x, int maxrecvsz){
    //int BCsendoffset;
    int sig = 1;
@precision SINGLE DOUBLE
    int data_ofset=my_flag_bc[0]*maxrecvsz;
@precision SCOMPLEX DCOMPLEX
    int data_ofset=my_flag_bc[0]*maxrecvsz*2;
@precision !

    for( int idxRecv = 0; idxRecv < tree->destCnt_; ++idxRecv ) {
        int iProc = tree->myDests_[idxRecv];
        //BCsendoffset = my_flag_bc[2];
        //double sum=0;
        //if (tid==0) {
        //    for(int i=0;i<my_flag_bc[3];i++){
        //        //printf("(%d), data, %d,%lf\n",mype,i,ready_x[i]);
        //        sum+=ready_x[my_flag_bc[2]+i];
        //    }
        //    printf("Start (%d), forwardDevice, send to %d, signal offset=%d, msgsz=%d,sum=%lf\n",mype,iProc,my_flag_bc[0],my_flag_bc[3],sum);
        //}
        //__syncthreads();
        //if (tid==0)
        //    printf("---- Start BC (%d), forwardDevice, send to %d, "
        //                  "signal offset=%d, "
        //                  "data offset=%d "
        //                  "msgsz=%d, maxrecvsz=%d\n",
        //                  mype,iProc,
        //                  my_flag_bc[0],
        //                  data_ofset,
        //                  my_flag_bc[1], maxrecvsz);
        //__syncthreads();
        //nvshmemx_double_put_block(&ready_x[BCsendoffset],ready_x,my_flag_bc[3],iProc);
        nvshmemx_double_put_nbi_block(&ready_x[data_ofset], &ready_x[data_ofset], my_flag_bc[1], iProc);
        nvshmem_fence();
        //__syncthreads();
        if (tid == 0) {
            nvshmemx_int_signal((int*)(flag_bc_q + my_flag_bc[0]), sig, iProc);
            //nvshmem_quiet();
            //printf("Done (%d), forwardDevice, send to %d, signal offset=%d, data offset=%d, msgsz=%d\n", mype, iProc,
            //       my_flag_bc[0], my_flag_bc[2], my_flag_bc[3]);

        }
    }
}

__device__ void C_RdTree_forwardMessageBlock_Device(C_Tree* Tree, int* flag_rd_q, int* my_flag_rd, int mype, int bid, int tid, double* ready_lsum, int maxrecvsz, int myroot){
    int data_ofset,sig_ofset;
    if (Tree->myIdx % 2 == 0) {
        sig_ofset = my_flag_rd[0] * 2;
        data_ofset = my_flag_rd[0] * maxrecvsz * 2;
    } else {
        sig_ofset = my_flag_rd[0] * 2 + 1;
        data_ofset = my_flag_rd[0] * maxrecvsz * 2 + maxrecvsz;
    }
    ////forward to my root if I have received everything
    //double sum = 0;
    //if (tid==0){
    //    for (int i = my_flag_rd[0]*maxrecvsz*2; i < my_flag_rd[0]*maxrecvsz*2 + my_flag_rd[1]; i++) {
    //        //printf("(%d), data, %d\n",mype,i);
    //        //printf("(%d), data, %d,%lf\n", mype, i, ready_lsum[i]);
    //        sum += ready_lsum[i];
    //    }
    //    printf("---- Start RD (%d,%d,%d), forwardMessage, forwardDevice, send to %d, "
    //       "lib=%d,size=%d,  dataoffset=%d,maxrecvsz=%d\n",
    //       mype, bid,tid, myroot,
    //       my_flag_rd[0], my_flag_rd[1], data_ofset,maxrecvsz);
    //}
    nvshmemx_double_put_nbi_block(&ready_lsum[data_ofset],&ready_lsum[my_flag_rd[0]*maxrecvsz*2],my_flag_rd[1]*scalefactor,myroot);
    //if (tid==0)
    //    printf("---- END RD (%d), forwardMessage, forwardDevice, send to %d, "
    //       "lib=%d,size=%d, sum=%lf, dataoffset=%d,maxrecvsz=%d\n",
    //       mype, myroot,
    //       my_flag_rd[0], my_flag_rd[1], sum, data_ofset,maxrecvsz);
    nvshmem_fence();
    int sig=1;
    if (tid==0)  nvshmemx_int_signal((int*)flag_rd_q+sig_ofset, sig, myroot);
    //if (tid==0)
    //    printf("Bsend:%d,%d,%d,%d,%d\n", mype, my_flag_rd[0],data_ofset, sig_ofset,my_flag_rd[1]);
}


__device__ void C_RdTree_forwardMessageWarp_Device(C_Tree* Tree, int* flag_rd_q, int* my_flag_rd, int mype, int bid, int tid, double* ready_lsum, int maxrecvsz, int myroot){
    int data_ofset,sig_ofset;
    if (Tree->myIdx % 2 == 0) {
        sig_ofset = my_flag_rd[0] * 2;
        data_ofset = my_flag_rd[0] * maxrecvsz * 2;
    } else {
        sig_ofset = my_flag_rd[0] * 2 + 1;
        data_ofset = my_flag_rd[0] * maxrecvsz * 2 + maxrecvsz;
    }

    ////forward to my root if I have received everything
    //double sum = 0;
    //if (tid%32==0) {
    //    for (int i = my_flag_rd[0]*maxrecvsz*2; i < my_flag_rd[0]*maxrecvsz*2 + my_flag_rd[1]; i++) {
    //        //printf("(%d), data, %d\n",mype,i);
    //        //printf("(%d), data, %d,%lf\n", mype, i, ready_lsum[i]);
    //        sum += ready_lsum[i];
    //    }
    //    printf("---- Start RD Warp (%d,%d,%d), forwardMessage, forwardDevice, send to %d, "
    //           "lib=%d,size=%d,  dataoffset=%d,maxrecvsz=%d\n",
    //           mype, bid, tid, myroot,
    //           my_flag_rd[0], my_flag_rd[1], data_ofset, maxrecvsz);
    //}

    nvshmemx_double_put_nbi_warp(&ready_lsum[data_ofset],&ready_lsum[my_flag_rd[0]*maxrecvsz*2],my_flag_rd[1]*scalefactor,myroot);
    //if (tid%32==0)
    //    printf("---- END RD Warp (%d), forwardMessage, forwardDevice, send to %d, "
    //       "lib=%d,size=%d, sum=%lf, dataoffset=%d,maxrecvsz=%d\n",
    //       mype, myroot,
    //       my_flag_rd[0], my_flag_rd[1], sum, data_ofset,maxrecvsz);
    nvshmem_fence();
    int sig=1;
    if (tid%32==0)  nvshmemx_int_signal((int*)flag_rd_q+sig_ofset, sig, myroot);
    //if (tid%32==0)
    //    printf("Wsend:%d,%d,%d,%d,%d\n", mype, my_flag_rd[0],data_ofset, sig_ofset,my_flag_rd[1]);

}

__device__ void C_RdTree_forwardMessageThread_Device(C_Tree* Tree, int* flag_rd_q, int* my_flag_rd, int mype, int bid, int tid, double* ready_lsum, int maxrecvsz, int myroot){
    int data_ofset,sig_ofset;
    if (Tree->myIdx % 2 == 0) {
        sig_ofset = my_flag_rd[0] * 2;
        data_ofset = my_flag_rd[0] * maxrecvsz * 2;
    } else {
        sig_ofset = my_flag_rd[0] * 2 + 1;
        data_ofset = my_flag_rd[0] * maxrecvsz * 2 + maxrecvsz;
    }

    ////forward to my root if I have received everything
    //double sum = 0;

    //for (int i = my_flag_rd[0]*maxrecvsz*2; i < my_flag_rd[0]*maxrecvsz*2 + my_flag_rd[1]; i++) {
    //    //printf("(%d), data, %d\n",mype,i);
    //    printf("(%d), data, %d,%lf\n", mype, i, ready_lsum[i]);
    //    sum += ready_lsum[i];
    //}

    //printf("---- Start RD Thread (%d,%d,%d), forwardMessage, forwardDevice, send to %d, "
    //       "lib=%d,size=%d,  dataoffset=%d,maxrecvsz=%d\n",
    //       mype, bid,tid, myroot,
    //       my_flag_rd[0], my_flag_rd[1], data_ofset,maxrecvsz);
    nvshmem_double_put_nbi(&ready_lsum[data_ofset],&ready_lsum[my_flag_rd[0]*maxrecvsz*2],my_flag_rd[1]*scalefactor,myroot);
    //printf("---- END RD Thread (%d,%d,%d), forwardMessage, forwardDevice, send to %d, "
    //       "lib=%d,size=%d,  dataoffset=%d,maxrecvsz=%d\n",
    //       mype, bid,tid, myroot,
    //       my_flag_rd[0], my_flag_rd[1], data_ofset,maxrecvsz);
    nvshmem_fence();
    int sig=1;
    nvshmemx_int_signal((int*)flag_rd_q+sig_ofset, sig, myroot);
    //printf("Tsend:%d,%d,%d,%d,%d\n",
    //       mype, my_flag_rd[0],data_ofset, sig_ofset,my_flag_rd[1]);
}

__global__ void wait_bcrd
(
int nrhs,
C_Tree  *LRtree_ptr,
int_t maxrecvsz,
int mype,
int* flag_bc_q,
int* flag_rd_q,
double* ready_x,
double* ready_lsum,
int* my_flag_bc,
int* my_flag_rd,
int* d_nfrecv,
int* d_status,
int* d_colnum,
int* d_mynum,
int* d_mymaskstart,
int* d_mymasklength,
int* d_nfrecvmod,
int* d_statusmod,
int* d_colnummod,
int* d_mynummod,
int* d_mymaskstartmod,
int* d_mymasklengthmod,
int* d_recv_cnt,
int* d_msgnum,
int* d_flag_mod,
@(type) *lsum,    /* Sum of local modifications.                        */
int_t *fmod,     /* Modification count for L-solve.                    */
gridinfo_t *grid,
int_t *xsup,
int_t *ilsum,
int nbrow_loc,
int_t  nsupers
) {

    int bid = blockIdx.x;
    //int global_id= blockIdx.x * blockDim.x * blockDim.y + threadIdx.x + threadIdx.y * blockDim.x;
    int tid = threadIdx.x + threadIdx.y * blockDim.x;
    int WAIT_NUM_THREADS = d_nfrecv[1]; //*d_nfrecv[2];
    //if (tid==0) printf("(%d) WAIT_NUM_THREADS=%d,tot_wait_col=%d\n",mype,WAIT_NUM_THREADS,d_nfrecv[0]);

    if (bid == 0) { // for BC recv
        if (WAIT_NUM_THREADS >= d_nfrecv[0]) {
            if (tid < d_nfrecv[0]) {
                nvshmem_int_wait_until((int *) flag_bc_q + d_colnum[tid], NVSHMEM_CMP_EQ, 1);
                d_status[d_colnum[tid]] = 1;
                //printf("WAIT1 (%d,%d) msg arrived in col %d\n", mype, tid, d_colnum[tid]);
            }
        } else {
            int delta = d_nfrecv[0] % WAIT_NUM_THREADS;
            if (tid < delta) {
                d_mynum[tid] = d_nfrecv[0] / WAIT_NUM_THREADS + 1;
            } else {
                d_mynum[tid] = d_nfrecv[0] / WAIT_NUM_THREADS;
            }
            __syncthreads();
            d_mymaskstart[tid] = 0;
            for (int i = 0; i < tid; i++) {
                d_mymaskstart[tid] += d_mynum[i];
            }
            d_mymasklength[tid] = d_colnum[d_mymaskstart[tid] + d_mynum[tid] - 1] - d_colnum[d_mymaskstart[tid]] + 1;
            __syncthreads();
            //printf("WAIT2 (%d,%d) mynum=%d, start=%d,%d length=%d\n",mype,tid,d_mynum[tid],d_mymaskstart[tid],d_colnum[d_mymaskstart[tid]],d_mymasklength[tid]);

            for (int i = 0; i < d_mynum[tid]; i++) {
                int wm_val = nvshmem_int_wait_until_any(flag_bc_q + d_colnum[d_mymaskstart[tid]], d_mymasklength[tid],
                                                        d_status + d_colnum[d_mymaskstart[tid]], NVSHMEM_CMP_EQ, 1);
                d_status[d_colnum[d_mymaskstart[tid]] + wm_val] = 1;
                //printf("WAIT2 (%d,%d) msg arrived in col %d, i=%d\n",mype,tid,d_colnum[d_mymaskstart[tid]] + wm_val, i);
            }
        }
   }
#if 0
   //if (tid==0) printf("(%d,%d,%d) WAIT EXIT\n",mype,bid,tid);
    if (bid == 1) { // for RD recv
        //if (tid==0) printf("RD---(%d) WAIT_NUM_THREADS=%d,tot_wait_col=%d\n",mype,WAIT_NUM_THREADS,d_nfrecvmod[1]);
        int j, iam, lib, mycol, myrow, k, knsupc, il, cnt;
        int_t fmod_tmp, aln_i;

        aln_i = 1;
        double temp;
        if (WAIT_NUM_THREADS >= d_nfrecvmod[1]) { // one thread wait for one col
            if (tid < d_nfrecvmod[1]) {
                //printf("(%d,%d,%d) d_colnummod=%d,recv_cnt=%d\n", mype, bid, tid, d_colnummod[tid], d_recv_cnt[d_colnummod[tid]]);
                for (int i = 0; i < d_recv_cnt[d_colnummod[tid]]; i++) {
                    //printf("(%d,%d,%d) d_colnummod=%d,recv_cnt=%d,i=%d,wait_off=%d,%d,status=%d,%d\n", mype, bid, tid, d_colnummod[tid], d_recv_cnt[d_colnummod[tid]],i,d_colnummod[tid]*2, d_colnummod[tid]*2+1,d_statusmod[d_colnummod[tid]*2], d_statusmod[d_colnummod[tid]*2+1]);
                    int wm_val = nvshmem_int_wait_until_any(flag_rd_q + d_colnummod[tid] * 2, 2,
                                                            d_statusmod + d_colnummod[tid] * 2, NVSHMEM_CMP_EQ, 1);
                    d_statusmod[d_colnummod[tid] * 2 + wm_val] = 1;
                    lib = (d_colnummod[tid] * 2 + wm_val) / 2;
                    iam = grid->iam;
                    mycol = MYCOL(iam, grid);
                    myrow = MYROW(iam, grid);
                    k = myrow + lib * grid->nprow; // global block row
                    knsupc = SuperSize(k);
                    il = LSUM_BLK(lib);
                    cnt = LRtree_ptr[lib].destCnt_;
                    //printf("recv1,%d,%d,%d,%d\n",
                    //       mype,d_colnummod[d_mymaskstartmod[tid]]*2,wm_val,lib);
                    //printf("(%d,%d,%d),idx=%d,lib=%d,cnt=%d\n", mype, bid, tid,
                    //       d_colnummod[tid] * 2 + wm_val, lib, cnt);
                    if (d_statusmod[lib * 2] + d_statusmod[lib * 2 + 1] == cnt) {
                        //double tmp_sum = 0;
                        int ii = 0;
                        if (cnt == 2) {
                            for (ii = 0; ii < cnt; ++ii) {
                                //tmp_sum = 0;
                                RHS_ITERATE(j) {
                                    for (int aab = 0; aab < knsupc; ++aab) {
                                        //temp=atomicAdd(&lsum[il+i + j*knsupc], ready_lsum[maxrecvsz*lib*2+ii*maxrecvsz + i + j*knsupc]  );
                                        temp = atomicAdd(&lsum[il + aab + j * knsupc],
                                                         ready_lsum[maxrecvsz * lib * 2 + ii * maxrecvsz + aab +
                                                                    j * knsupc]);
                                        //tmp_sum += ready_lsum[maxrecvsz * lib * 2 + ii * maxrecvsz + aab + j * knsupc];
                                        //printf("data2-(%d,%d,%d),lib=%d,k=%d,ii=%d,sum=%lf,ready_lsum[%d]=%f\n", mype, bid, tid,
                                        //       lib, k, ii, tmp_sum,
                                        //       maxrecvsz * lib * 2 + ii * maxrecvsz + i + j * knsupc,
                                        //       ready_lsum[maxrecvsz * lib * 2 + ii * maxrecvsz + i + j * knsupc]);
                                    }

                                    // atomic return old val
                                    fmod_tmp = atomicSub(&fmod[lib * aln_i], 1);
                                    //printf("sum2-(%d,%d,%d),lib=%d,k=%d,sum=%f,fmod_tmp=%d, tmp_sum=%lf\n", mype, bid, tid, lib, k,
                                    //       tmp_sum,fmod_tmp, tmp_sum);
                                    //printf("sum2-(%d,%d,%d),lib=%d,k=%d,fmod_tmp=%d\n", mype, bid, tid, lib, k,fmod_tmp);
                                }
                            }
                        }
                        if (cnt == 1) {
                            if (flag_rd_q[lib * 2 + 1] == 1) ii = 1;
                            //tmp_sum = 0;
                            RHS_ITERATE(j) {
                                for (int aab = 0; aab < knsupc; ++aab) {
                                    //temp=atomicAdd(&lsum[il+i + j*knsupc], ready_lsum[maxrecvsz*lib*2+ii*maxrecvsz + i + j*knsupc]  );
                                    temp = atomicAdd(&lsum[il + aab + j * knsupc],
                                                     ready_lsum[maxrecvsz * lib * 2 + ii * maxrecvsz + aab + j * knsupc]);
                                    //tmp_sum += ready_lsum[maxrecvsz * lib * 2 + ii * maxrecvsz + aab + j * knsupc];
                                    //printf("data1-(%d,%d,%d),lib=%d,k=%d,ii=%d,sum=%lf,ready_lsum[%d]=%lf\n", mype, bid, tid, lib, k, ii,
                                    //       tmp_sum,maxrecvsz * lib * 2 + ii * maxrecvsz + aab + j * knsupc,
                                    //       ready_lsum[maxrecvsz * lib * 2 + ii * maxrecvsz + aab + j * knsupc]);
                                }

                            }
                            // atomic return old val
                            fmod_tmp = atomicSub(&fmod[lib * aln_i], 1);
                            //printf("sum1-(%d,%d,%d),lib=%d,k=%d,fmod_tmp=%d\n", mype, bid, tid, lib, k,fmod_tmp);
                            //printf("sum1-(%d,%d,%d),lib=%d,k=%d,sum=%f,fmod_tmp=%d\n", mype, bid, tid, lib, k, tmp_sum,fmod_tmp);
                        }

                        if (fmod_tmp == 1) {// forward RD
                            //senddone[lk]=1;
                            if (LRtree_ptr[lib].myRoot_ != LRtree_ptr[lib].myRank_) {
                                //cnt=LRtree_ptr[lib].msgSize_;
                                my_flag_rd[k * RDMA_FLAG_SIZE] = lib;
                                my_flag_rd[k * RDMA_FLAG_SIZE + 1] = LRtree_ptr[lib].msgSize_;
                                double tmp_sum=0;
                                RHS_ITERATE(j) {
                                    for (int aab = 0; aab < knsupc; aab++) {
                                        ready_lsum[lib * maxrecvsz * 2 + aab + j * knsupc] = lsum[il + aab + j * knsupc];
                                        //tmp_sum += ready_lsum[lib * maxrecvsz * 2 + aab + j * knsupc];
                                        //printf("data3-(%d,%d,%d),lib=%d,k=%d,i=%d,ready_lsum[%d]=%f\n", mype, bid, tid, lib, k, i,
                                        //       k * maxrecvsz * 2 + i +j * knsupc,
                                        //       ready_lsum[k * maxrecvsz * 2 + i +j * knsupc]);

                                    }
                                }
                                //printf("(%d,%d,%d),in wait lib=%d,k=%d,myflagrd=%d,%d\n", mype, bid, tid, lib, k,
                                //       my_flag_rd[k * RDMA_FLAG_SIZE], my_flag_rd[k * RDMA_FLAG_SIZE + 1]);
                                int temp_mysendcout=atomicAdd(&d_flag_mod[0], 1);
                                int temp_flag_mod=atomicExch(&d_flag_mod[temp_mysendcout+1],lib);
                                //printf("iam=%d in wait,lib=%d,%d,%d, pos=%d, temp %d,%d\n",mype,lib,k, d_flag_mod[temp_mysendcout+1], temp_mysendcout+1, temp_mysendcout,temp_flag_mod);
                                //printf("iam=%d in wait,lib=%d,%d,%d, pos=%d, temp %d,%d, sum=%lf\n",mype,lib,k, d_flag_mod[temp_mysendcout+1], temp_mysendcout+1, temp_mysendcout,temp_flag_mod, tmp_sum);
                                //C_RdTree_forwardMessageSimple_Device(&LRtree_ptr[lib], (int *) flag_rd_q,
                                //                                     &my_flag_rd[RDMA_FLAG_SIZE * k], mype, bid, tid,
                                //                                     &ready_lsum[0], maxrecvsz);
                            }
                        }
                    }
                }//for
            }
        } else {
            int delta = d_nfrecvmod[1] % WAIT_NUM_THREADS;
            //int mynum = d_nfrecvmod[1] / WAIT_NUM_THREADS;
            //int mystart = tid*(mynum+1);
            //if (tid < delta) {
            //    mynum = mynum + 1;
            //    //d_mynummod[tid] = d_nfrecvmod[1] / WAIT_NUM_THREADS+1;
            //}else{
            //    mystart = (delta*(mynum+1))+(tid-delta)*mynum;
            //}
            //int mymasklength=(d_colnummod[mystart + mynum - 1] - d_colnummod[mystart]+1)*2;

            if (tid < delta){
                d_mynummod[tid] = d_nfrecvmod[1] / WAIT_NUM_THREADS + 1;
            }else {
                d_mynummod[tid] = d_nfrecvmod[1] / WAIT_NUM_THREADS;
            }
            __syncthreads();

            d_mymaskstartmod[tid] = 0;
            d_mymasklengthmod[tid] = 0;
            d_msgnum[tid] = 0;

            ////d_mymaskstartmod: start offset of d_colnummod
            for (int i = 0; i < tid; i++) {
                d_mymaskstartmod[tid] += d_mynummod[i];
                //printf("(%d,%d,%d),i=%d,d_mynummod=%d,d_mymaskstartmod=%d\n",
                //       mype,bid,tid,i,
                //       d_mynummod[i],d_mymaskstartmod[tid]);
            }
            __syncthreads();

            for (int i = d_mymaskstartmod[tid]; i < d_mymaskstartmod[tid] + d_mynummod[tid]; i++) {
                d_msgnum[tid] += d_recv_cnt[d_colnummod[i]];
                //printf("(%d,%d,%d),i=%d,d_recv_cnt=%d\n",mype,bid,tid,i,d_recv_cnt[d_colnummod[i]]);
            }
            d_mymasklengthmod[tid] = (d_colnummod[d_mymaskstartmod[tid] + d_mynummod[tid] - 1]
                                      - d_colnummod[d_mymaskstartmod[tid]]+1)*2;

            //printf("(%d,%d,%d) waitcol=%d,msgnum=%d,masklength=%d,start=%d\n",mype,bid,tid,
            //                   d_mynummod[tid],d_msgnum[tid],
            //                   d_mymasklengthmod[tid],d_mymaskstartmod[tid]);

            //printf("(%d,%d), mynum=%d,%d,mystart=%d,%d, mylength=%d,%d\n",mype,tid,d_mynummod[tid], mynum,d_mymaskstartmod[tid], mystart, d_mymasklengthmod[tid],mymasklength);
            for (int i = 0; i < d_msgnum[tid]; i++) {
                //printf("(%d,%d,%d)--before wait any,i=%d/%d\n",mype,bid,tid,i,d_msgnum[tid]);
                int wm_val = nvshmem_int_wait_until_any(&flag_rd_q[d_colnummod[d_mymaskstartmod[tid]] * 2],
                                                        d_mymasklengthmod[tid],
                                                        &d_statusmod[d_colnummod[d_mymaskstartmod[tid]] * 2],
                                                        NVSHMEM_CMP_EQ, 1);
                d_statusmod[d_colnummod[d_mymaskstartmod[tid]]*2 + wm_val] = 1;
                lib = (d_colnummod[d_mymaskstartmod[tid]]*2 + wm_val) / 2;
                //printf("recv,%d,%d,%d,%d,%d\n",
                //       mype,tid,d_colnummod[d_mymaskstartmod[tid]]*2,wm_val,lib);
                iam = grid->iam;
                mycol = MYCOL(iam, grid);
                myrow = MYROW(iam, grid);

                k = myrow + lib * grid->nprow; // global block row
                knsupc = SuperSize(k);
                il = LSUM_BLK(lib);
                cnt = LRtree_ptr[lib].destCnt_;
                //printf("HERE2-(%d,%d,%d),lib=%d,k=%d,wm_val=%d,cnt=%d,%d, mycnt=%d\n", mype, bid, tid, lib, k,
                //       wm_val,cnt,d_recv_cnt[lib],d_statusmod[lib * 2] + d_statusmod[lib * 2 + 1]);

                if (d_statusmod[lib * 2] + d_statusmod[lib * 2 + 1] == cnt) {
                    //double tmp_sum = 0;
                    int ii = 0;
                    if (cnt == 2) {
                        for (ii = 0; ii < cnt; ++ii) {
                            //tmp_sum = 0;
                            RHS_ITERATE(j) {
                                for (int aab = 0; aab < knsupc; aab++) {
                                    //temp=atomicAdd(&lsum[il+i + j*knsupc], ready_lsum[maxrecvsz*lib*2+ii*maxrecvsz + i + j*knsupc]  );
                                    temp = atomicAdd(&lsum[il + aab + j * knsupc],
                                                     ready_lsum[maxrecvsz * lib * 2 + ii * maxrecvsz + aab +
                                                                j * knsupc]);
                                    //tmp_sum += ready_lsum[maxrecvsz * lib * 2 + ii * maxrecvsz + aab + j * knsupc];
                                    //printf("data2-(%d,%d,%d),lib=%d,k=%d,ii=%d,ready_lsum[%d]=%f\n", mype, bid, tid,
                                    //       lib, k, ii,
                                    //       maxrecvsz * lib * 2 + ii * maxrecvsz + i + j * knsupc,
                                    //       ready_lsum[maxrecvsz * lib * 2 + ii * maxrecvsz + i + j * knsupc]);
                                }

                                // atomic return old val
                                fmod_tmp = atomicSub(&fmod[lib * aln_i], 1);
                                //printf("sum2-(%d,%d,%d),lib=%d,k=%d,sum=%f,fmod_tmp=%d\n", mype, bid, tid, lib, k,tmp_sum,fmod_tmp);
                            }
                        }
                    }
                    if (cnt == 1) {
                        if (flag_rd_q[k * 2 + 1] == 1) ii = 1;
                        RHS_ITERATE(j) {
                            for (int aab = 0; aab < knsupc; ++aab) {
                                temp = atomicAdd(&lsum[il + aab + j * knsupc],
                                                 ready_lsum[maxrecvsz * lib * 2 + ii * maxrecvsz + aab + j * knsupc]);
                                //tmp_sum += ready_lsum[maxrecvsz * lib * 2 + ii * maxrecvsz + aab + j * knsupc];
                                //printf("data1-(%d,%d,%d),lib=%d,k=%d,ii=%d,ready_lsum[%d]=%f\n", mype, bid, tid, lib, k, ii,
                                //       maxrecvsz * lib * 2 + ii * maxrecvsz + i + j * knsupc,
                                //       ready_lsum[maxrecvsz * lib * 2 + ii * maxrecvsz + i + j * knsupc]);
                            }

                        }
                        // atomic return old val
                        fmod_tmp = atomicSub(&fmod[lib * aln_i], 1);
                        //printf("sum1-(%d,%d,%d),lib=%d,k=%d,sum=%f,fmod_tmp=%d\n", mype, bid, tid, lib, k, tmp_sum,fmod_tmp);
                    }

                    if (fmod_tmp == 1) {// forward RD
                        //printf("sum1-(%d,%d,%d),lib=%d, myRoot=%d\n", mype, bid, tid, lib,LRtree_ptr[lib].myRoot_);
                        if (LRtree_ptr[lib].myRoot_ != LRtree_ptr[lib].myRank_) {
                            my_flag_rd[k * RDMA_FLAG_SIZE] = lib;
                            my_flag_rd[k * RDMA_FLAG_SIZE + 1] = LRtree_ptr[lib].msgSize_;
                            RHS_ITERATE(j) {
                                for (int aab = 0; aab < knsupc; aab++) {
                                    ready_lsum[lib * maxrecvsz * 2 + aab + j * knsupc] = lsum[il + aab + j * knsupc];
                                    //printf("data3-(%d,%d,%d),lib=%d,k=%d,i=%d,ready_lsum[%d]=%f\n", mype, bid, tid, lib, k, i,
                                    //       k * maxrecvsz * 2 + i +j * knsupc,
                                    //       ready_lsum[k * maxrecvsz * 2 + i +j * knsupc]);

                                }
                            }
                            //printf("(%d,%d,%d),in wait lib=%d,k=%d,myflagrd=%d,%d\n", mype, bid, tid, lib, k,
                            //       my_flag_rd[k * RDMA_FLAG_SIZE], my_flag_rd[k * RDMA_FLAG_SIZE + 1]);
                            int temp_mysendcout=atomicAdd(&d_flag_mod[0], 1);
                            int temp_flag_mod=atomicExch(&d_flag_mod[temp_mysendcout+1],lib);
                            //printf("iam=%d in wait2,lib=%d,%d,%d, pos=%d, temp %d,%d\n",mype,lib,k, d_flag_mod[temp_mysendcout+1], temp_mysendcout+1, temp_mysendcout,temp_flag_mod);
                            //C_RdTree_forwardMessageSimple_Device(&LRtree_ptr[lib], (int *) flag_rd_q,
                            //                                     &my_flag_rd[RDMA_FLAG_SIZE * k], mype, bid, tid,
                            //                                     &ready_lsum[0], maxrecvsz);
                        }
                    }
                }
            }//for
        } // else WAIT_NUM_THREAD<recv
    }

    if (bid==2){
        int tot_threads=blockDim.x * blockDim.y;
        //if (tid==0){
        //    printf("iam=%d, len=%d, tot_threads=%d\n",mype,d_nfrecvmod[3],tot_threads);
        //}

        //if (d_nfrecvmod[3]==0) return;
        int lk=-1,k=-1,iam=-1,myroot=-1,myrank=-1;
        int mycol,myrow,lib;
        __shared__ int recv_num, finish_num;
        __shared__ int cur_send_num;
        recv_num = finish_num = 0;

        for (int i=1; i<d_nfrecvmod[3]+1;i=i+cur_send_num){

            if (tid==0){
                int tmp, tmp1;
                //printf("iam=%d,i=%d, count=%d\n",mype,i,d_flag_mod[0]);
                do {
                    tmp = d_flag_mod[0];
                    //tmp1 == d_flag_mod[tmp];
                    __threadfence();
                    //msg_recv=d_status[gc];
                    //msg_recv=flag_bc_q[gc];
                } while (tmp == finish_num);

                recv_num=tmp;
            }
            __syncthreads();
            cur_send_num=recv_num-finish_num;
            finish_num=recv_num;

            if ((cur_send_num <= tot_threads/32)){
                if (tid/32 < cur_send_num){
                    lk=d_flag_mod[i+tid/32];
                    //if (tid%32==0) printf("-- Warp, (%d,%d) i=%d, recv_num=%d,cur_send_num=%d, lk=%d, size=%d\n",mype,tid,i, recv_num,cur_send_num,lk,my_flag_rd[RDMA_FLAG_SIZE*k+1]);
                    iam = grid->iam;
                    mycol = MYCOL(iam, grid);
                    myrow = MYROW(iam, grid);
                    k = myrow + lk * grid->nprow; // global block row
                    myroot=LRtree_ptr[lk].myRoot_;
                    myrank=LRtree_ptr[lk].myRank_;
                    //if (tid%32==0) printf("W, (%d,%d) loop=%d, recv_num=%d,cur_send_num=%d, k=%d, to %d\n",mype,tid,i, recv_num,cur_send_num, lk, myroot);
                    C_RdTree_forwardMessageWarp_Device(&LRtree_ptr[lk], (int*)flag_rd_q, &my_flag_rd[RDMA_FLAG_SIZE*k], mype, bid, tid, &ready_lsum[0],maxrecvsz,myroot);
                    //if (tid%32==0) printf("W Done, (%d,%d) loop=%d, recv_num=%d,cur_send_num=%d, lk=%d\n",mype,tid,i, recv_num,cur_send_num,lk);
                }
            }else if ((cur_send_num > tot_threads/32) && (cur_send_num <= tot_threads)){
                __syncthreads();
                if (tid < cur_send_num){
                    lk=d_flag_mod[i+tid];
                    iam = grid->iam;
                    mycol = MYCOL(iam, grid);
                    myrow = MYROW(iam, grid);
                    k = myrow + lk * grid->nprow; // global block row
                    myroot=LRtree_ptr[lk].myRoot_;
                    myrank=LRtree_ptr[lk].myRank_;
                    //printf("-- Thread, (%d,%d) i=%d, recv_num=%d,cur_send_num=%d, lk=%d, size=%d\n",mype,tid,i, recv_num,cur_send_num,lk,my_flag_rd[RDMA_FLAG_SIZE*k+1]);
                    C_RdTree_forwardMessageThread_Device(&LRtree_ptr[lk], (int*)flag_rd_q, &my_flag_rd[RDMA_FLAG_SIZE*k], mype, bid, tid, &ready_lsum[0],maxrecvsz,myroot);
                    //printf("T Done,(%d,%d) recv_num=%d,cur_send_num=%d\n",mype,tid, recv_num,cur_send_num);
                }
            }else if (cur_send_num > tot_threads){
                int delta=cur_send_num%tot_threads;
                int mynum=cur_send_num/tot_threads;
                int myoffset=0;
                if (tid < delta){
                    myoffset=tid*(mynum+1);
                }else{
                    myoffset=(delta*(mynum+1))+(tid-delta)*mynum;
                }

                for(int j=0;j<mynum;j++){
                    lk=d_flag_mod[i+myoffset+j];
                    iam = grid->iam;
                    mycol = MYCOL(iam, grid);
                    myrow = MYROW(iam, grid);
                    k = myrow + lk * grid->nprow; // global block row
                    myroot=LRtree_ptr[lk].myRoot_;
                    myrank=LRtree_ptr[lk].myRank_;
                    //printf("-- Threadloop, (%d,%d) i=%d, recv_num=%d,cur_send_num=%d, lk=%d, size=%d\n",mype,tid,i, recv_num,cur_send_num,lk,my_flag_rd[RDMA_FLAG_SIZE*k+1]);
                    C_RdTree_forwardMessageThread_Device(&LRtree_ptr[lk], (int*)flag_rd_q, &my_flag_rd[RDMA_FLAG_SIZE*k], mype, bid, tid, &ready_lsum[0],maxrecvsz,myroot);
                    //printf("Ts Done,(%d,%d) loop=%d (%d,%d) recv_num=%d,cur_send_num=%d, k=%d, to %d\n",mype, tid,i, j, mynum,recv_num,cur_send_num,lk,myroot);
                }
            }

            __syncthreads();

            //if (tid==0) printf("iam=%d,tid=%d,i=%d, recv_num=%d,cur_send_num=%d\n",mype,tid,i,recv_num,cur_send_num);
        }
    }
#endif

}



/************************************************************************/
/*! \brief
 *
 * <pre>
 * Purpose
 * =======
 *   Perform local block modifications: lsum[i] -= L_i,k * X[k] on multi-GPU.
 * </pre>
 */
__global__ void dlsum_fmod_inv_gpu_mrhs_nvshmem
/************************************************************************/
        (
                int_t nbcol_loc,
                @(type) *lsum,    /* Sum of local modifications.                        */
                @(type)  *x,       /* X array (local)                                    */
                int   nrhs,      /* Number of right-hand sides.                        */
                int   maxsup,      /* Max supernode size.                        */
                int_t   nsupers,      /* Number of total supernodes.                        */
                int_t *fmod,     /* Modification count for L-solve.                    */
                C_Tree  *LBtree_ptr,
                C_Tree  *LRtree_ptr,
                int_t *ilsum,
                int_t *Lrowind_bc_dat,
                long int *Lrowind_bc_offset,
                @(type) *Lnzval_bc_dat,
                long int *Lnzval_bc_offset,
                @(type) *Linv_bc_dat,
                long int *Linv_bc_offset,
                int_t *Lindval_loc_bc_dat,
                long int *Lindval_loc_bc_offset,
                int_t *xsup,
                gridinfo_t *grid,
                int_t maxrecvsz,
                int mype,
                volatile int* flag_bc_q,
                volatile int* flag_rd_q,
                double* ready_x,
                double* ready_lsum,
                int* my_flag_bc,
                int* my_flag_rd,
                int* d_nfrecv,
                volatile int* d_status,
                volatile int* d_statusmod,
                int_t nblock_ex,
                int* d_flag_mod
        )
{
    @precision SINGLE DOUBLE
        @(type) alpha = 1.0;
        @(type) beta = 0.0;
        @(type) zero = 0.0;
        @(type) malpha = -1.0;
    @precision SCOMPLEX DCOMPLEX
        @(type) alpha = {1.0, 0.0};
        @(type) beta = {0.0, 0.0};
        @(type) malpha = {-1.0, 0.0};
        @(type) zero = {0.0, 0.0};
    @precision !

    @(type) *lusup, *lusup1;
    @(type) *dest;
    @(type) *Linv;/* Inverse of diagonal block */

    int    iam, iknsupc, myrow, mycol, krow, nbrow, nbrow1, nbrow_ref, nsupr, nsupr1, p, pi, idx_r,m;
    int_t  k,i, l,ii,jj, ik, il, ikcol, irow, j, lb, lk, rel, lib,lready;
    int_t  *lsub, *lsub1, nlb1, lptr1, luptr1,*lloc;
    int_t  luptr_tmp,luptr_tmp1,lptr1_tmp, idx_i, idx_v,idx_n,  idx_l, fmod_tmp, lbstart,lbend,nn,Nchunk,nlb_loc,remainder;
    int thread_id1;
    flops_t ops_loc=0.0;
    MPI_Status status;
    int test_flag;
    yes_no_t done;
    int_t* idx_lsum,idx_lsum1;
    const int Nbk=1;
    __shared__ double rtemp_loc[128];
    @(type) temp,temp1;
    int_t ldalsum;
    int_t nleaf_send_tmp;
    int_t lptr;      /* Starting position in lsub[*].                      */
    int_t luptr;     /* Starting position in lusup[*].                     */
    int_t iword = sizeof(int_t);
    int_t dword = sizeof (double);
    int_t aln_d,aln_i;
    aln_d = 1;//ceil(CACHELINE/(double)dword);
    aln_i = 1;//ceil(CACHELINE/(double)iword);
    int   knsupc;    /* Size of supernode k.                               */
    int_t nlb;       /* Number of L blocks.                                */

    int_t bid=blockIdx_x;
    int_t tmp;
    int_t tid = threadIdx_x + threadIdx_y * blockDim_x;
    int_t ready = 0;
    // int_t lock = 0;
    const int block_size = blockDim_x*blockDim_y; /* number of threads per warp*/
    @(type) rC[THR_N][THR_M];
    gpuError_t error;
    int_t idx = threadIdx_x;  // thread's m dimension
    int_t idy = threadIdx_y;  // thread's n dimension
    int_t ni,mi;
    int cnt;
    yes_no_t test;
    // rtemp_loc = (double*)malloc(maxsup*nrhs*Nbk*sizeof(double));
    //int delta=(nblock_ex < nbcol_loc ? nblock_ex : nbcol_loc);
    //int_t bid1;
    //bool flag=false;
    //if (bid<(delta*2)){
    //    bid1=bid/2;
    //    flag= bid%2==0 ? 1:0;
    //}else{
    //    bid1=bid-delta;
    //    flag= delta==nblock_ex ? 1:0;
    //}
    //if(tid==0) printf("(%d) iam bid=%d,bid1=%d,flag=%d\n",mype,bid,bid1,flag);
    //if(tid==0) printf("(%d) iam bid=%d,enter solve 0\n",mype,bid);

    //if(bid<nbcol_loc){
    if (Lrowind_bc_offset[bid] == -1) {
       return;
    }
    //if(tid==0) printf("(%d) iam bid=%d,enter solve--1\n",mype,bid);
    int get_offset, get_msgsize, get_rank, gc, gr, tmp_id, recv_offset = 0;

    lk = bid;
    iam = grid->iam;
    mycol = MYCOL(iam, grid);
    myrow = MYROW(iam, grid);
    gc = mycol + lk * grid->npcol;
    if (gc >= nsupers) return;
    k = gc; //mycol + lk * grid->npcol;

    knsupc = SuperSize(k);
    lsub = &Lrowind_bc_dat[Lrowind_bc_offset[lk]];
    iam = grid->iam;
    krow = PROW(k, grid);
    lusup = &Lnzval_bc_dat[Lnzval_bc_offset[lk]];
    lloc = &Lindval_loc_bc_dat[Lindval_loc_bc_offset[lk]];
    nsupr = lsub[1];

    if (myrow == krow) {
        nlb = lsub[0] - 1;
        idx_n = 1;
        idx_i = nlb + 2;
        idx_v = 2 * nlb + 3;
        luptr_tmp = lloc[idx_v];
        m = nsupr - knsupc;
    } else {
        nlb = lsub[0];
        idx_n = 0;
        idx_i = nlb;
        idx_v = 2 * nlb;
        luptr_tmp = lloc[idx_v];
        m = nsupr;
    }


    // printf("  Before kernel:   %i %i %i %i %i %i %i %i\n", threadIdx_x, blockIdx_x, grid->npcol, nsupers,myrow,krow,bid,tid);
    if (myrow == krow) {   /* diagonal block performs trsm and forward the message*/

        if (tid == 0) {  /*only the first thread in a block handles the lock */
            //printf("(%d) iam bid=%d,enter solve--2, wait lock,gc=%d\n",mype,bid,gc);
            //printf("bk: %5d r: %5d %5d %5d\n",mycol+bid*grid->npcol,fmod[2*aln_i],myrow,krow);
            // for (i=0 ; i<maxsup ; i++){
            // rtemp_loc[i]=0.0;
            // }

            lib = LBi(k, grid); /* Local block number, row-wise. */
            do {
                tmp = fmod[lib * aln_i];
                __threadfence();
            } while (tmp > 0);
        }
        __syncthreads();
        //if(tid==0) printf("(%d) iam bid=%d,enter solve--2, unlock,gc=%d\n",mype,bid,gc);

        lib = LBi(k, grid); /* Local block number, row-wise. */
        il = LSUM_BLK(lib);
        ii = X_BLK(lib);
 @precision SINGLE DOUBLE
        RHS_ITERATE(j)for (i = tid; i < knsupc; i += block_size) {
                        //atomicAdd(&ready_x[0],lsum[i + il + j * knsupc]);
                        x[i + ii + j * knsupc] += lsum[i + il + j * knsupc];
                    }
 @precision SCOMPLEX DCOMPLEX
               RHS_ITERATE(j)for (i = tid; i < knsupc; i += block_size) {
                               //atomicAdd(&ready_x[0],lsum[i + il + j * knsupc]);
                               x[i + ii + j * knsupc].r += lsum[i + il + j * knsupc].r;
                               x[i + ii + j * knsupc].i += lsum[i + il + j * knsupc].i;
                               }
 @precision !

        __syncthreads();
        //if(tid==0) printf("(%d,%d,%d),CHECKING k=%d,gc=%d,checksum=%lf\n",mype,bid,tid,k,gc,ready_x[0]);

        //  if(Llu->inv == 1){

        Linv = &Linv_bc_dat[Linv_bc_offset[lk]];
// double: two branches
// doublecomplex: 1 nrhs
        if (nrhs == 1) {

            for (i = tid; i < knsupc; i += block_size) {
                temp1 = zero;
                for (l = 0; l < knsupc; l++) {

@precision SINGLE DOUBLE
                    temp1 += Linv[l * knsupc + i] * x[ii + l];
@precision SCOMPLEX DCOMPLEX
                    zz_mult(&temp1, &Linv[l * knsupc + i], &x[ii + l]);
@precision !
                }
@precision SINGLE DOUBLE
                lsum[il + i] = temp1; //reuse lsum as temporary output as it's no longer accessed
@precision SCOMPLEX DCOMPLEX
                z_copy(&lsum[il + i], &temp1); //reuse lsum as temporary output as it's no longer accessed
@precision !
            }
            __syncthreads();

            for (i = tid; i < knsupc; i += block_size) {

@precision SINGLE DOUBLE
                x[i + ii] = lsum[il + i];
@precision SCOMPLEX DCOMPLEX
                z_copy(&x[i + ii],&lsum[il + i]);
@precision !
                // printf("lk %5d %lf\n",lk,x[i + ii + j*knsupc]);
            }
            __syncthreads();

           // RHS_ITERATE(j){

           // for (i = tid; i < knsupc; i+=block_size)
           // rtemp_loc[i]=zero;
           // __syncthreads();


           // gemv_device_dlsum_fmod(
           // knsupc, knsupc, alpha,
           // Linv, knsupc,
           // &x[ii+j*knsupc], 1, beta,
           // rtemp_loc, 1);

           // __syncthreads();
           // // printf("tid %5d knsupc %5d block_size %5d\n",tid,knsupc,block_size);
           // for (i = tid; i < knsupc; i+=block_size){
           // x[i + ii + j*knsupc] = rtemp_loc[i];
           // // printf("lk %5d %lf\n",lk,x[i + ii + j*knsupc]);
           // }
           // }
           // __syncthreads();

        } else {
           __syncthreads();
           for (int_t blx = 0; blx * BLK_M < knsupc; blx++) {
               for (int_t bly = 0; bly * BLK_N < nrhs; bly++) {
                   gemm_device_dlsum_fmod(knsupc, nrhs, knsupc, blx, bly,
                                          Linv, knsupc, &x[ii], knsupc, rC,
                                          alpha, beta);
#pragma unroll
                   for (ni = 0; ni < THR_N; ni++) {
                       int_t coord_dCn = bly * BLK_N + ni * DIM_Y + idy;
#pragma unroll
                       for (mi = 0; mi < THR_M; mi++) {
                           int_t coord_dCm = blx * BLK_M + mi * DIM_X + idx;
                           if (coord_dCm < knsupc && coord_dCn < nrhs) {
                               double &regC = rC[ni][mi];
@precision SINGLE DOUBLE
                               lsum[coord_dCm + il + coord_dCn * knsupc] = regC;  //reuse lsum as temporary output as it's no longer accessed
@precision SCOMPLEX DCOMPLEX
                               z_copy(&lsum[coord_dCm + il + coord_dCn * knsupc], &regC);
@precision !
                           }//if (coord_dCm < knsupc && coord_dCn < nrhs)
                       }
                   }
               }
           }
           __syncthreads();

           RHS_ITERATE(j)for (i = tid; i < knsupc; i += block_size)
@precision SINGLE DOUBLE
                   x[i + ii + j * knsupc] = lsum[i + il + j * knsupc];
@precision SCOMPLEX DCOMPLEX
                   z_copy(&x[i + ii + j * knsupc],&lsum[i + il + j * knsupc]);
@precision !
           __syncthreads();
        }//if(nrhs==1)

        RHS_ITERATE(j)for (i = tid; i < knsupc; i += block_size)
@precision SINGLE DOUBLE
                ready_x[i + maxrecvsz * lk + j * knsupc] = x[i + ii + j * knsupc];
@precision SCOMPLEX DCOMPLEX
                ready_x[(i + maxrecvsz * lk + j * knsupc)*2] = x[i + ii + j * knsupc].r;
                ready_x[(i + maxrecvsz * lk + j * knsupc)*2+1] = x[i + ii + j * knsupc].i;
@precision !

        __syncthreads();
    } else {   /* off-diagonal block forward the message*/
            /* waiting for the x subvector and forward*/
            //YL: only the first thread in a block spin-waits for the coming x subvector message using NVSHMEM, put the message into ready_x[maxrecvsz*lk]
        volatile int msg_recv = 0;
        if (tid == 0) {
            //printf("in solve WAIT1 (%d,%d) wait for col %d,flag=%d\n", mype, bid, gc,flag_bc_q[gc]);
            //nvshmem_int_wait_until((int *) flag_bc_q + gc, NVSHMEM_CMP_EQ, 1);
            do {
                msg_recv = flag_bc_q[lk];
                //msg_recv=d_status[gc];
                //msg_recv=flag_bc_q[gc];
                __threadfence();
            } while (msg_recv != 1);
            //printf("(%d,%d,%d,%d) in compute kernel, I have msg=%d,sz=%d,ofset=%d\n",mype,bid,tid,gc,msg_recv,LBtree_ptr[lk].msgSize_*nrhs+XK_H,maxrecvsz*lk);
            //double sum=0;
            //for (int myi=0;myi<LBtree_ptr[lk].msgSize_*nrhs+XK_H;myi++){
            //    sum+=ready_x[maxrecvsz*lk+myi];
            //}
            //printf("(%d,%d,%d), gc=%d,lk=%d, sum=%lf\n",mype,bid,tid,gc,lk,sum);
        }
        __syncthreads();
        //for(int i=0;i<LBtree_ptr[lk].msgSize_*nrhs+XK_H;i++){
        //    ready_x[maxrecvsz*lk+i]=ready_x[maxrecvsz*gc+i];
        //}
        //__syncthreads();
    }
    __syncthreads();

    //YL: only the first thread in a block forwards the x subvector using NVSHMEM
    cnt = LBtree_ptr[lk].destCnt_;
    if (cnt > 0) {
        //cnt=LBtree_ptr[lk].msgSize_;
        my_flag_bc[gc * RDMA_FLAG_SIZE] = lk;
@precision SINGLE DOUBLE
        my_flag_bc[gc * RDMA_FLAG_SIZE + 1] = LBtree_ptr[lk].msgSize_ * nrhs + XK_H;
@precision SCOMPLEX DCOMPLEX
        my_flag_bc[gc * RDMA_FLAG_SIZE + 1] = LBtree_ptr[lk].msgSize_ *2* nrhs + XK_H;
@precision !
        C_BcTree_forwardMessageSimple_Device(&LBtree_ptr[lk], (int *) flag_bc_q, &my_flag_bc[gc * RDMA_FLAG_SIZE],
                                             mype, tid, &ready_x[0], maxrecvsz);
        //printf("(%d,%d,%d), lk=%d, gc=%d\n",mype,bid,tid,lk,gc);
        //C_BcTree_forwardMessageSimple_Device(&LBtree_ptr[lk],&ready_x[maxrecvsz*lk],cnt*nrhs+XK_H);
    }
    int keep_lk = lk;
    __syncthreads();

    if (nlb > 0) {

        lib = LBi(k, grid); /* Local block number, row-wise. */
        ii = X_BLK(lib);

        if (nrhs == 1) {
            luptr_tmp1 = lloc[idx_v];
            lb = 0;
            nbrow = 0;
            lptr1_tmp = lloc[lb + idx_i];
            lptr = lptr1_tmp + 2;
            nbrow1 = lsub[lptr1_tmp + 1];
            ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
            rel = xsup[ik]; /* Global row index of block ik. */
            lk = LBi(ik, grid); /* Local block number, row-wise. */
            iknsupc = SuperSize(ik);
            il = LSUM_BLK(lk);

            for (i = tid; i < m; i += block_size) {
                while (nbrow + lsub[lptr1_tmp + 1] <= i) {
                    lb++;
                    nbrow += lsub[lptr1_tmp + 1];
                    lptr1_tmp = lloc[lb + idx_i];
                    lptr = lptr1_tmp + 2;
                    ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
                    rel = xsup[ik]; /* Global row index of block ik. */
                    lk = LBi(ik, grid); /* Local block number, row-wise. */
                    iknsupc = SuperSize(ik);
                    il = LSUM_BLK(lk);
                }

                irow = lsub[lptr + i - nbrow] - rel; /* Relative row. */
                RHS_ITERATE(j) {
                    temp1 = zero;
                    for (l = 0; l < knsupc; l++) {
@precision SINGLE DOUBLE
                        temp1 += lusup[luptr_tmp1 + l * nsupr + i] * ready_x[l + maxrecvsz * keep_lk + j * knsupc];
@precision SCOMPLEX DCOMPLEX
                        z_add(&temp1,&temp1,&lusup[luptr_tmp1 + l * nsupr + i] * ready_x[l + maxrecvsz * keep_lk + j * knsupc]);
@precision !
                        //temp1+= lusup[luptr_tmp1+l*nsupr+i]*x[ii+j*knsupc+l];
                    }
@precision SINGLE DOUBLE
                    temp = atomicAdd(&lsum[il + irow + j * iknsupc], -temp1);
@precision SCOMPLEX DCOMPLEX
                    temp.r = atomicAdd(&lsum[il + irow + j * iknsupc].r, -temp1.r);
                    temp.i = atomicAdd(&lsum[il + irow + j * iknsupc].i, -temp1.i);
@precision !
                    //printf("(%d,%d,%d),lsum[%d]=%f\n",mype,bid,tid,il+irow + j*iknsupc,lsum[il+irow + j*iknsupc]);
                }

                //  irow = lsub[lptr+i-nbrow] - rel; /* Relative row. */
                //  if(i==nbrow+lsub[lptr1_tmp+1]-1){
                // 	 fmod_tmp=atomicSub(&fmod[lk*aln_i],1);
                // 	 // __threadfence();
                //  }


            }
            __syncthreads();

            luptr_tmp1 = lloc[idx_v];
            lb = 0;
            nbrow = 0;
            lptr1_tmp = lloc[lb + idx_i];
            lptr = lptr1_tmp + 2;
            nbrow1 = lsub[lptr1_tmp + 1];
            ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
            rel = xsup[ik]; /* Global row index of block ik. */
            lk = LBi(ik, grid); /* Local block number, row-wise. */
            iknsupc = SuperSize(ik);
            il = LSUM_BLK(lk);
            gr=myrow + lk * grid->nprow;
            //knsupc = SuperSize(gr);

            for (i = tid; i < m; i += block_size) {
                while (nbrow + lsub[lptr1_tmp + 1] <= i) {
                    lb++;
                    nbrow += lsub[lptr1_tmp + 1];
                    lptr1_tmp = lloc[lb + idx_i];
                    lptr = lptr1_tmp + 2;
                    ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
                    rel = xsup[ik]; /* Global row index of block ik. */
                    lk = LBi(ik, grid); /* Local block number, row-wise. */
                    iknsupc = SuperSize(ik);
                    il = LSUM_BLK(lk);
                }
                //if (ik==15) printf("(%d) iam bid=%d,enter solve--3,fmod=%d\n",mype,bid,fmod_tmp);

                irow = lsub[lptr + i - nbrow] - rel; /* Relative row. */
                if (i == nbrow + lsub[lptr1_tmp + 1] - 1) {
                    // atomic return old val, omp return new val
                    fmod_tmp = atomicSub(&fmod[lk * aln_i], 1);
                    //printf("(%d) iam bid=%d,tid=%d,enter solve--6,i=%d,r=%d,lk=%d,ik=%d,fmod_tmp=%d,fmod=%d\n",mype,bid,tid,i,nbrow + lsub[lptr1_tmp + 1] - 1, lk,ik,fmod_tmp,fmod[lk * aln_i]);
                    // __threadfence();
                    if(fmod_tmp==1) {// forward RD
                        //senddone[lk]=1;
                        if(LRtree_ptr[lk].myRoot_ != LRtree_ptr[lk].myRank_){
                            //cnt=LRtree_ptr[lib].msgSize_;

                            my_flag_rd[ik*RDMA_FLAG_SIZE]=lk;
@precision SINGLE DOUBLE
                            my_flag_rd[ik*RDMA_FLAG_SIZE+1]=LRtree_ptr[lk].msgSize_;
@precision SCOMPLEX DCOMPLEX
                            my_flag_rd[ik*RDMA_FLAG_SIZE+1]=LRtree_ptr[lk].msgSize_*2;
@precision !
                            //double tmp_sum=0;
                            RHS_ITERATE(j) {
                                for (int aab = 0; aab < iknsupc; aab++) {

@precision SINGLE DOUBLE
                                    ready_lsum[lk * maxrecvsz * 2 + aab +j * iknsupc] = lsum[il + aab +j * iknsupc];
@precision SCOMPLEX DCOMPLEX
                                    ready_lsum[(lk * maxrecvsz * 2 + aab +j * iknsupc)*2] = lsum[il + aab +j * iknsupc].r;
                                    ready_lsum[(lk * maxrecvsz * 2 + aab +j * iknsupc)*2+1] = lsum[il + aab +j * iknsupc].i;
@precision !
                                    //tmp_sum += ready_lsum[lk * maxrecvsz * 2 + aab +j * iknsupc];
                                    //printf("data3-(%d,%d,%d),lib=%d,k=%d,%d,i=%d,sum=%lf,ready_lsum[%d]=%lf, size=%d\n", mype, bid, tid, lk, gr,ik, i, tmp_sum,
                                    //       lk * maxrecvsz * 2 + aab +j * iknsupc,
                                    //       ready_lsum[lk * maxrecvsz * 2 + aab +j * iknsupc],my_flag_rd[ik*RDMA_FLAG_SIZE+1]);

                                }
                            }
                            int temp_mysendcout=atomicAdd(&d_flag_mod[0], 1);
                            int temp_flag_mod=atomicExch(&d_flag_mod[temp_mysendcout+1],lk);
                            //printf("iam=%d in solve,lib=%d,%d,%d, "
                            //       "pos=%d, temp %d,%d, "
                            //       "maxrecvsz=%d\n",mype,lk,k, d_flag_mod[temp_mysendcout+1],
                            //       temp_mysendcout+1,
                            //       temp_mysendcout,temp_flag_mod,
                            //       maxrecvsz);
                            //printf("(%d,%d,%d) in solve,lib=%d,gr=%d,ik=%d,myflagrd=%d,%d\n",mype,bid,tid,lk,gr,ik,my_flag_rd[ik*RDMA_FLAG_SIZE],my_flag_rd[ik*RDMA_FLAG_SIZE+1]);
                            //C_RdTree_forwardMessageSimple_Device(&LRtree_ptr[lk], (int*)flag_rd_q, &my_flag_rd[RDMA_FLAG_SIZE*ik], mype, bid, tid, &ready_lsum[0],maxrecvsz);
                        }
                    }
                }
            }
            //__syncthreads();

        } else {
            for (lb = 0; lb < nlb; lb++) {
                luptr_tmp1 = lloc[lb + idx_v];

                // nbrow=0;
                // lptr1_tmp = lloc[lb+idx_i];
                // nbrow += lsub[lptr1_tmp+1];


                lib = LBi(k, grid); /* Local block number, row-wise. */
                ii = X_BLK(lib);

                lptr1_tmp = lloc[lb + idx_i];
                lptr = lptr1_tmp + 2;
                nbrow1 = lsub[lptr1_tmp + 1];
                ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
                rel = xsup[ik]; /* Global row index of block ik. */

                lk = LBi(ik, grid); /* Local block number, row-wise. */

                iknsupc = SuperSize(ik);
                il = LSUM_BLK(lk);


                // if(nrhs==1){

                // for (i = tid; i < nbrow1; i+=block_size)
                // rtemp_loc[i]=zero;
                // __syncthreads();


                // gemv_device_dlsum_fmod(
                // nbrow1, knsupc, alpha,
                // &lusup[luptr_tmp1], nsupr,
                // &x[ii], 1, beta,
                // rtemp_loc, 1);

                // __syncthreads();
                // for (i = tid; i < nbrow1; i+=block_size){
                // irow = lsub[lptr+i] - rel; /* Relative row. */
                // temp=atomicAdd(&lsum[il+irow],-rtemp_loc[i]);
                // }
                // }else{

                for (int_t blx = 0; blx * BLK_M < nbrow1; blx++) {
                    for (int_t bly = 0; bly * BLK_N < nrhs; bly++) {
                        gemm_device_dlsum_fmod(nbrow1, nrhs, knsupc, blx, bly,
                                               &lusup[luptr_tmp1], nsupr, &ready_x[maxrecvsz * keep_lk], knsupc, rC,
                                               alpha, beta);
#pragma unroll
                            for (ni = 0; ni < THR_N; ni++) {
                                int_t coord_dCn = bly * BLK_N + ni * DIM_Y + idy;
#pragma unroll
                                for (mi = 0; mi < THR_M; mi++) {
                                    int_t coord_dCm = blx * BLK_M + mi * DIM_X + idx;
                                    if (coord_dCm < nbrow1 && coord_dCn < nrhs) {
                                        irow = lsub[lptr + coord_dCm] - rel; /* Relative row. */
                                        @(type) &regC = rC[ni][mi];
@precision SINGLE DOUBLE
                                        temp = atomicAdd(&lsum[il + irow + coord_dCn * iknsupc], -regC);
@precision SCOMPLEX DCOMPLEX
                                        temp.r = atomicAdd(&lsum[il + irow + coord_dCn * iknsupc].r, -regC.r);
                                        temp.i = atomicAdd(&lsum[il + irow + coord_dCn * iknsupc].i, -regC.i);
@precision !
                                    }
                                }
                            }
                        }
                    }
                    // }//if(nrhs==1)

                    if (tid == 0) fmod_tmp = atomicSub(&fmod[lk * aln_i], 1);


                }

            }//if(nrhs==1)


            // if(tid==0){
            // for (lb = tid; lb < nlb; lb+=block_size){
            // lptr1_tmp = lloc[lb+idx_i];
            // ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
            // lk = LBi( ik, grid ); /* Local block number, row-wise. */
            // fmod_tmp=atomicSub(&fmod[lk*aln_i],1);
            // // printf("k: %5d r: %5d\n",mycol+bid*grid->npcol,fmod[2*aln_i]);
            // }
            // }
            //__syncthreads();
            // } /*if tid<Nchunk*/
        } /* if nlb>0*/
} /* dlsum_fmod_inv_gpu_mrhs */



void dlsum_fmod_inv_gpu_wrap
        (
                int_t nbcol_loc,    /*number of local supernode columns*/
                int_t nbrow_loc,    /*number of local supernode rows*/
                int_t nthread_x,     /*kernel launch parameter*/
                int_t nthread_y,     /*kernel launch parameter*/
                @(type) *lsum,    /* Sum of local modifications.                        */
                @(type) *x,       /* X array (local)                                    */
                int   nrhs,      /* Number of right-hand sides.                        */
                int   maxsup,      /* Max supernode size.                        */
                int_t   nsupers,      /* Number of total supernodes.                        */
                int_t *fmod,     /* Modification count for L-solve.                    */
                C_Tree  *LBtree_ptr,
                C_Tree  *LRtree_ptr,
                int_t *ilsum,
                int_t *Lrowind_bc_dat,
                long int *Lrowind_bc_offset,
                @(type) *Lnzval_bc_dat,
                long int *Lnzval_bc_offset,
                @(type) *Linv_bc_dat,
                long int *Linv_bc_offset,
                int_t *Lindval_loc_bc_dat,
                long int *Lindval_loc_bc_offset,
                int_t *xsup,
                gridinfo_t *grid,
                int_t maxrecvsz,
                int* flag_bc_q,
                int* flag_rd_q,
                double* ready_x,
                double* ready_lsum,
                int* my_flag_bc,
                int* my_flag_rd,
                int* d_nfrecv,
                int* h_nfrecv,
                int* d_status,
                int* d_colnum,
                int* d_mynum,
                int* d_mymaskstart,
                int* d_mymasklength,
                int* d_nfrecvmod,
                int* d_statusmod,
                int* d_colnummod,
                int* d_mynummod,
                int* d_mymaskstartmod,
                int* d_mymasklengthmod,
                int* d_recv_cnt,
                int* d_msgnum,
                int* d_flag_mod
        ) {

    gpuStream_t sid = 0;
    int gid = 0;
    int mycol;
    int_t lk, k, knsupc;
    int_t nblock_ex = CEILING(nbrow_loc, ((nthread_x * nthread_y) / 32)); //32 (warp) * 8 =256

    int mype, npes, ndevices;
    mype = nvshmem_my_pe();
    npes = nvshmem_n_pes();
    int mype_node = nvshmem_team_my_pe(NVSHMEMX_TEAM_NODE);
    CUDA_CHECK(cudaSetDevice(mype_node));


    cudaStream_t stream[2];
    for (int i = 0; i < 2; ++i) {
        //cudaStreamCreate(&stream[i]);
        cudaStreamCreateWithFlags(&stream[i], cudaStreamNonBlocking);
    }

    int minGridSize, myblockSize;
    cudaOccupancyMaxPotentialBlockSize(&minGridSize,&myblockSize,(const void *) wait_bcrd ,0,0 );
    if (myblockSize < h_nfrecv[1]) {
        h_nfrecv[1] = myblockSize;
        gpuMemcpy(d_nfrecv, h_nfrecv, 3 * sizeof(int), gpuMemcpyHostToDevice);
    }
    //printf("(%d) solve=%d,%d, wait_bcrd=2,%d (%d, %d), send_rd=1,%d\n",
    //       mype,nbcol_loc,nthread_x*nthread_y,
    //       h_nfrecv[1],minGridSize, myblockSize, h_nfrecv[1]);
    //fflush(stdout);


    dim3 dimGrid_bc(1); //3
    dim3 dimGrid_rd(1); //3
    dim3 dimBlock_bc(h_nfrecv[1]); //256 by default
    dim3 dimGrid(nbcol_loc);
    dim3 dimBlock(nthread_x, nthread_y);

    int launch_success = 0;

    void *args[] = {&nrhs, &LRtree_ptr, &maxrecvsz, &mype, &flag_bc_q, &flag_rd_q,
                    &ready_x, &ready_lsum, &my_flag_bc, &my_flag_rd, &d_nfrecv, &d_status,
                    &d_colnum, &d_mynum, &d_mymaskstart, &d_mymasklength,
                    &d_nfrecvmod, &d_statusmod, &d_colnummod, &d_mynummod, &d_mymaskstartmod, &d_mymasklengthmod,
                    &d_recv_cnt, &d_msgnum, &d_flag_mod, &lsum,&fmod,&grid,&xsup,&ilsum,&nbrow_loc,&nsupers};



    int status=1;
    status = nvshmemx_collective_launch((const void *) wait_bcrd, dimGrid_bc, dimBlock_bc, args, 0, stream[0]);
    //status1 = nvshmemx_collective_launch((const void *) send_rd, dimGrid_rd, dimBlock_bc, args, 0, stream[1]);
    //printf("(%d), status=%d,%d\n",mype, status,status1);
    //fflush(stdout);

    if ((status != NVSHMEMX_SUCCESS)) {
        fprintf(stderr, "shmemx_collective_launch failed %d,%d\n", status);
        exit(-1);
    } else{
       dlsum_fmod_inv_gpu_mrhs_nvshmem<<< dimGrid, dimBlock, 0, stream[1] >>>(nbcol_loc,
                                                                           lsum, x,
                                                                           nrhs, maxsup, nsupers,
                                                                           fmod,
                                                                           LBtree_ptr, LRtree_ptr,
                                                                           ilsum,
                                                                           Lrowind_bc_dat,
                                                                           Lrowind_bc_offset,
                                                                           Lnzval_bc_dat,
                                                                           Lnzval_bc_offset,
                                                                           Linv_bc_dat,
                                                                           Linv_bc_offset,
                                                                           Lindval_loc_bc_dat,
                                                                           Lindval_loc_bc_offset,
                                                                           xsup,
                                                                           grid, maxrecvsz,
                                                                           mype, flag_bc_q,
                                                                           flag_rd_q,
                                                                           ready_x, ready_lsum,
                                                                           my_flag_bc, my_flag_rd,
                                                                           d_nfrecv, d_status,
                                                                           d_statusmod,nblock_ex,d_flag_mod);
    //CUDA_CHECK(cudaGetLastError());
    }
    CUDA_CHECK(cudaDeviceSynchronize());

}
#ifdef __cplusplus
}
#endif